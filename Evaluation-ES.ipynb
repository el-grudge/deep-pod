{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7294031f-f87c-4037-bbae-bb18f73e96cc",
   "metadata": {},
   "source": [
    "# evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dad00ed2-34ef-4172-9f74-a59cc9a703bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minasonbol/Documents/study/deep-pod/.venv/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.exceptions import AuthenticationException, ConnectionError\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ffec9d-4d76-4f7a-a8e4-efa12850565b",
   "metadata": {},
   "source": [
    "## Load the .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21eeb820-82e3-4c22-857f-f917614a8d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "elasticsearch_cloud_id = os.getenv('ES_CLOUD_ID')\n",
    "elasticsearch_api_key = os.getenv('ES_API_KEY')\n",
    "replicate_api_key = os.getenv('REPLICATE_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebc5a81-a2ed-4755-9b9e-242ad1c336db",
   "metadata": {},
   "source": [
    "## index functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a713daa2-fc5d-4cdb-9e69-6b6c59928314",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_es_index(client, index_name):\n",
    "    # Create mapping\n",
    "    index_settings = {\n",
    "        \"settings\": {\n",
    "            \"number_of_shards\": 1,\n",
    "            \"number_of_replicas\": 0\n",
    "        },\n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"text\": {\"type\": \"text\"},\n",
    "                \"text_vector\": {\"type\": \"dense_vector\", \"dims\": 768},\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    client.indices.delete(index=index_name, ignore_unavailable=True)\n",
    "    client.indices.create(index=index_name, body=index_settings)\n",
    "    \n",
    "    return client.indices.get_alias(index=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cc14612-170a-4887-9004-49f231121fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_t5_embedding(encoder, chunks):\n",
    "    documents = []\n",
    "\n",
    "    for sentence in chunks:\n",
    "        temp_dict = {\n",
    "            'text': sentence['text'],\n",
    "            'text_vector': encoder.encode(sentence[\"text\"]).tolist()\n",
    "        }\n",
    "        documents.append(temp_dict)\n",
    "    \n",
    "    return {'documents': documents}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6257d048-6aed-4e5d-b5da-d170969cdfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_es_index(documents, index_name, client):\n",
    "    # add documents \n",
    "    for doc in documents:\n",
    "        try:\n",
    "            client.index(index=index_name, body=doc)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    return index_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c38dec0-84f9-4277-a2d0-c21f8e7e7030",
   "metadata": {},
   "source": [
    "## RAG functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53506e9b-aa53-4f28-8074-0b47910fd993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search\n",
    "def search(query, **kwargs):\n",
    "\n",
    "    # Encode the query\n",
    "    query_vector = kwargs['encoder'].encode(query).tolist()\n",
    "\n",
    "    # Construct the search query\n",
    "    search_query = {\n",
    "        \"size\": kwargs['num_results'],  # Limit the number of results\n",
    "        \"query\": {\n",
    "            \"script_score\": {\n",
    "                \"query\": {\n",
    "                    \"match_all\": {}\n",
    "                },\n",
    "                \"script\": {\n",
    "                    \"source\": \"cosineSimilarity(params.query_vector, 'text_vector') + 1.0\",\n",
    "                    \"params\": {\n",
    "                        \"query_vector\": query_vector\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    # Execute the search query\n",
    "    results = kwargs['vector_db_client'].search(index=kwargs['index_name'], body=search_query)\n",
    "    results = results['hits']['hits']\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0f391f9-04c4-496a-b2b5-10a46399dd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt\n",
    "def build_prompt(query, search_results):\n",
    "    prompt_template = \"\"\"\n",
    "    You're a podcast chat bot. Answer the QUESTION based on the CONTEXT from the RESULTS database.\n",
    "    Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "\n",
    "    QUESTION: {question}\n",
    "    \n",
    "    CONTEXT: \n",
    "    {context}\n",
    "    \"\"\".strip()\n",
    "    \n",
    "    context = \"\"\n",
    "    \n",
    "    for search_result in search_results:\n",
    "        doc = search_result['_source']['text'] if '_source' in search_result.keys() else search_result['text']\n",
    "        context = context + f\"{doc}\\n\\n\"\n",
    "    \n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "233592e2-2257-4f31-b75c-09b1e4383102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate\n",
    "def llm(prompt, **kwargs):\n",
    "    inputs = kwargs['llm_tokenizer'](prompt, return_tensors=\"pt\")\n",
    "    outputs = kwargs['llm_client'].generate(\n",
    "        inputs[\"input_ids\"], \n",
    "        max_length=100,\n",
    "        num_beams=5,\n",
    "        do_sample=True,\n",
    "        temperature=1.0,\n",
    "        top_k=50,\n",
    "        top_p=0.95,    \n",
    "        )\n",
    "    response = kwargs['llm_tokenizer'].decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fdb108b9-b2e0-4fe8-b3ad-273ef69fdf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rag \n",
    "def rag(query, **kwargs):\n",
    "\n",
    "    search_results = search(\n",
    "        query, \n",
    "        encoder=kwargs['encoder'] if 'encoder' in kwargs.keys() else None, \n",
    "        index_name=kwargs['index_name'], \n",
    "        vector_db_client=kwargs['vector_db_client'] if 'vector_db_client' in kwargs.keys() else None,        \n",
    "        num_results=5\n",
    "        )\n",
    "\n",
    "    prompt = build_prompt(query, search_results)\n",
    "\n",
    "    answer = llm(\n",
    "        prompt, \n",
    "        llm_client=kwargs['llm_client'], \n",
    "        llm_tokenizer=kwargs['llm_tokenizer'] if 'llm_tokenizer' in kwargs.keys() else None\n",
    "        )\n",
    "    \n",
    "    for word in answer.split():\n",
    "        yield word + \" \"\n",
    "        time.sleep(0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bb87f7-8c9b-4610-b3b9-94f9331e20d9",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "936057b4-20d3-4889-9194-b5eaec59b7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# podcast option\n",
    "episode_option = \"1. Try a sample\"\n",
    "episode_option_selected=True    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cea6e7f1-fc41-4cd8-a143-ec0ee3e5f6aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minasonbol/Documents/study/deep-pod/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# sentence encoder\n",
    "sentence_encoder = \"1. T5\"\n",
    "encoder=SentenceTransformer(\"sentence-transformers/sentence-t5-base\")\n",
    "sentence_encoder_selected=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a7571a8-9790-4d3e-a5ac-71a5a7c4faf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transcription method\n",
    "# skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed7d258a-84eb-4f82-b824-8beaa8635a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index podcast-transcriber was created successfully.\n"
     ]
    }
   ],
   "source": [
    "# vector database\n",
    "index_name = \"podcast-transcriber\"\n",
    "vector_db=\"2. Elasticsearch\"\n",
    "if elasticsearch_api_key != '' and elasticsearch_cloud_id != '':\n",
    "    try:\n",
    "        vector_db_client = Elasticsearch(cloud_id=elasticsearch_cloud_id, api_key=elasticsearch_api_key)\n",
    "        response = vector_db_client.cluster.health()\n",
    "        index=create_es_index(vector_db_client, index_name)\n",
    "        vector_db_selected=True\n",
    "        index_created=True\n",
    "        print(f\"Index {[k for k,v in index.items()][0]} was created successfully.\")\n",
    "    except AuthenticationException:\n",
    "        print(\"Invalid API key or Cloud ID. Please provide a valid tokens.\")\n",
    "    except ConnectionError:\n",
    "        print(\"Connection error. Could not connect to the cluster.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2fd0db11-a5ac-47cb-a4ce-b3ea0927c2e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "# llm\n",
    "llm_option = \"2. FLAN-5\"\n",
    "llm_client = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\")\n",
    "llm_tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "llm_option_selected=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4af5b39f-b6ce-4d6a-959c-cb786f38f530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download\n",
    "with open('sample/episode_details.json', 'r') as f:\n",
    "    episode_details = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "286cf9ee-bc68-4234-9c51-d9ad5ec741de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transcribe\n",
    "# skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9773f523-6a7d-4a64-8d3b-664910d49e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode\n",
    "episode_details.update(create_t5_embedding(encoder, episode_details['chunks']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "436564c7-1642-4ab1-acb2-a01208f91de1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'podcast-transcriber'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# populate index\n",
    "populate_es_index(episode_details['documents'], index_name, vector_db_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b98ada0-bf69-4fa7-8d45-fa0c2ad8bd16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marxists \n"
     ]
    }
   ],
   "source": [
    "# interact\n",
    "query = \"Who were the Mensheviks?\"\n",
    "response = rag(\n",
    "    query, \n",
    "    encoder=encoder,\n",
    "    index_name=index_name,\n",
    "    vector_db_client=vector_db_client,\n",
    "    num_results=5,\n",
    "    llm_client=llm_client,\n",
    "    llm_tokenizer=llm_tokenizer\n",
    "    )\n",
    "print(\" \".join(list(response)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fb3625-ccad-4522-8d52-b7aa53e38f2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
