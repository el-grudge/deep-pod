{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7294031f-f87c-4037-bbae-bb18f73e96cc",
   "metadata": {},
   "source": [
    "# evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dad00ed2-34ef-4172-9f74-a59cc9a703bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/bin/miniconda3/envs/TalAI2/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ffec9d-4d76-4f7a-a8e4-efa12850565b",
   "metadata": {},
   "source": [
    "## Load the .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21eeb820-82e3-4c22-857f-f917614a8d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "replicate_api_key = os.getenv('REPLICATE_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebc5a81-a2ed-4755-9b9e-242ad1c336db",
   "metadata": {},
   "source": [
    "## index functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cc14612-170a-4887-9004-49f231121fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_t5_embedding(encoder, chunks):\n",
    "    documents = []\n",
    "\n",
    "    for sentence in chunks:\n",
    "        temp_dict = {\n",
    "            'text': sentence['text'],\n",
    "            'text_vector': encoder.encode(sentence[\"text\"]).tolist()\n",
    "        }\n",
    "        documents.append(temp_dict)\n",
    "    \n",
    "    return {'documents': documents}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5582528e-e4c0-4f5d-bc9f-bb0bbe812fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_chroma_collection(documents, collection):\n",
    "    ids = [str(i) for i in range(len(documents))]\n",
    "    embeddings = [doc['text_vector'] for doc in documents]\n",
    "    texts = [doc['text'] for doc in documents]\n",
    "    \n",
    "    collection.add(\n",
    "        ids=ids,\n",
    "        embeddings=embeddings,\n",
    "        metadatas=[{\"text\": text} for text in texts]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c38dec0-84f9-4277-a2d0-c21f8e7e7030",
   "metadata": {},
   "source": [
    "## RAG functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef2c328b-96fe-4195-8dec-391c9afd468e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, **kwargs):\n",
    "    query_vector = kwargs['encoder'].encode(query).tolist()\n",
    "    \n",
    "    # Perform cosine similarity search in ChromaDB\n",
    "    results = kwargs['vector_db_client'].get_or_create_collection(kwargs['index_name']).query(\n",
    "        query_embeddings=[query_vector],\n",
    "        n_results=kwargs['num_results']\n",
    "    )\n",
    "    \n",
    "    return results[\"metadatas\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0f391f9-04c4-496a-b2b5-10a46399dd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt\n",
    "def build_prompt(query, search_results):\n",
    "    prompt_template = \"\"\"\n",
    "    You're a podcast chat bot. Answer the QUESTION based on the CONTEXT from the RESULTS database.\n",
    "    Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "\n",
    "    QUESTION: {question}\n",
    "    \n",
    "    CONTEXT: \n",
    "    {context}\n",
    "    \"\"\".strip()\n",
    "    \n",
    "    context = \"\"\n",
    "    \n",
    "    for search_result in search_results:\n",
    "        doc = search_result['_source']['text'] if '_source' in search_result.keys() else search_result['text']\n",
    "        context = context + f\"{doc}\\n\\n\"\n",
    "    \n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "233592e2-2257-4f31-b75c-09b1e4383102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate\n",
    "def llm(prompt, **kwargs):\n",
    "    inputs = kwargs['llm_tokenizer'](prompt, return_tensors=\"pt\")\n",
    "    outputs = kwargs['llm_client'].generate(\n",
    "        inputs[\"input_ids\"], \n",
    "        max_length=100,\n",
    "        num_beams=5,\n",
    "        do_sample=True,\n",
    "        temperature=1.0,\n",
    "        top_k=50,\n",
    "        top_p=0.95,    \n",
    "        )\n",
    "    response = kwargs['llm_tokenizer'].decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdb108b9-b2e0-4fe8-b3ad-273ef69fdf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rag \n",
    "def rag(query, **kwargs):\n",
    "\n",
    "    search_results = search(\n",
    "        query, \n",
    "        encoder=kwargs['encoder'] if 'encoder' in kwargs.keys() else None, \n",
    "        index_name=kwargs['index_name'], \n",
    "        vector_db_client=kwargs['vector_db_client'] if 'vector_db_client' in kwargs.keys() else None,        \n",
    "        num_results=5\n",
    "        )\n",
    "\n",
    "    prompt = build_prompt(query, search_results)\n",
    "\n",
    "    answer = llm(\n",
    "        prompt, \n",
    "        llm_client=kwargs['llm_client'], \n",
    "        llm_tokenizer=kwargs['llm_tokenizer'] if 'llm_tokenizer' in kwargs.keys() else None\n",
    "        )\n",
    "    \n",
    "    for word in answer.split():\n",
    "        yield word + \" \"\n",
    "        time.sleep(0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bb87f7-8c9b-4610-b3b9-94f9331e20d9",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "936057b4-20d3-4889-9194-b5eaec59b7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# podcast option\n",
    "episode_option = \"1. Try a sample\"\n",
    "episode_option_selected=True    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cea6e7f1-fc41-4cd8-a143-ec0ee3e5f6aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/bin/miniconda3/envs/TalAI2/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# sentence encoder\n",
    "sentence_encoder = \"1. T5\"\n",
    "encoder=SentenceTransformer(\"sentence-transformers/sentence-t5-base\")\n",
    "sentence_encoder_selected=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a7571a8-9790-4d3e-a5ac-71a5a7c4faf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transcription method\n",
    "# skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "109e6954-fa11-403a-ab18-c8d2baf3c6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector database\n",
    "index_name = \"podcast-transcriber\"\n",
    "vector_db=\"3. ChromaDB\"\n",
    "# Initialize ChromaDB client\n",
    "vector_db_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "\n",
    "# Create collection (equivalent to index in ES)\n",
    "index = vector_db_client.create_collection(\n",
    "    name=index_name,\n",
    "    metadata={\"distance_metric\": \"cosine\"} # defaults to euclidean distance\n",
    ")\n",
    "\n",
    "vector_db_selected=True\n",
    "index_created=True\n",
    "# print(f\"Index {[k for k,v in index.items()][0]} was created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2fd0db11-a5ac-47cb-a4ce-b3ea0927c2e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "# llm\n",
    "llm_option = \"2. FLAN-5\"\n",
    "llm_client = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\")\n",
    "llm_tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "llm_option_selected=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4af5b39f-b6ce-4d6a-959c-cb786f38f530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download\n",
    "with open('sample/episode_details.json', 'r') as f:\n",
    "    episode_details = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "286cf9ee-bc68-4234-9c51-d9ad5ec741de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transcribe\n",
    "# skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9773f523-6a7d-4a64-8d3b-664910d49e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode\n",
    "episode_details.update(create_t5_embedding(encoder, episode_details['chunks']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "436564c7-1642-4ab1-acb2-a01208f91de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# populate index\n",
    "populate_chroma_collection(episode_details['documents'], index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c544d5d7-db4c-41d0-85f7-f3a199721c02",
   "metadata": {},
   "source": [
    "## search didn't retrieve anything!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0b98ada0-bf69-4fa7-8d45-fa0c2ad8bd16",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 12\u001b[0m\n\u001b[1;32m      2\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWho were the Mensheviks?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m response \u001b[38;5;241m=\u001b[39m rag(\n\u001b[1;32m      4\u001b[0m     query, \n\u001b[1;32m      5\u001b[0m     encoder\u001b[38;5;241m=\u001b[39mencoder,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     llm_tokenizer\u001b[38;5;241m=\u001b[39mllm_tokenizer\n\u001b[1;32m     11\u001b[0m     )\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mlist\u001b[39m(response)))\n",
      "Cell \u001b[0;32mIn[8], line 12\u001b[0m, in \u001b[0;36mrag\u001b[0;34m(query, **kwargs)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrag\u001b[39m(query, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m      4\u001b[0m     search_results \u001b[38;5;241m=\u001b[39m search(\n\u001b[1;32m      5\u001b[0m         query, \n\u001b[1;32m      6\u001b[0m         encoder\u001b[38;5;241m=\u001b[39mkwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoder\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoder\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m         num_results\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m\n\u001b[1;32m     10\u001b[0m         )\n\u001b[0;32m---> 12\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m build_prompt(query, search_results)\n\u001b[1;32m     14\u001b[0m     answer \u001b[38;5;241m=\u001b[39m llm(\n\u001b[1;32m     15\u001b[0m         prompt, \n\u001b[1;32m     16\u001b[0m         llm_client\u001b[38;5;241m=\u001b[39mkwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mllm_client\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[1;32m     17\u001b[0m         llm_tokenizer\u001b[38;5;241m=\u001b[39mkwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mllm_tokenizer\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mllm_tokenizer\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m         )\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m answer\u001b[38;5;241m.\u001b[39msplit():\n",
      "Cell \u001b[0;32mIn[6], line 16\u001b[0m, in \u001b[0;36mbuild_prompt\u001b[0;34m(query, search_results)\u001b[0m\n\u001b[1;32m     13\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m search_result \u001b[38;5;129;01min\u001b[39;00m search_results:\n\u001b[0;32m---> 16\u001b[0m     doc \u001b[38;5;241m=\u001b[39m search_result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_source\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_source\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m search_result\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;28;01melse\u001b[39;00m search_result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     17\u001b[0m     context \u001b[38;5;241m=\u001b[39m context \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdoc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     19\u001b[0m prompt \u001b[38;5;241m=\u001b[39m prompt_template\u001b[38;5;241m.\u001b[39mformat(question\u001b[38;5;241m=\u001b[39mquery, context\u001b[38;5;241m=\u001b[39mcontext)\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "# interact\n",
    "query = \"Who were the Mensheviks?\"\n",
    "response = rag(\n",
    "    query, \n",
    "    encoder=encoder,\n",
    "    index_name=index_name,\n",
    "    vector_db_client=vector_db_client,\n",
    "    num_results=5,\n",
    "    llm_client=llm_client,\n",
    "    llm_tokenizer=llm_tokenizer\n",
    "    )\n",
    "print(\" \".join(list(response)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fb3625-ccad-4522-8d52-b7aa53e38f2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
